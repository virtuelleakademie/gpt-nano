@online{190407272Introduction2021,
  title = {[1904.07272] {{Introduction}} to {{Multi-Armed Bandits}}},
  date = {2021-01-31},
  url = {https://web.archive.org/web/20210131110323/https://arxiv.org/abs/1904.07272},
  urldate = {2023-04-21},
  file = {/Users/andrew/Zotero/storage/W6I62VZC/1904.html}
}

@online{adamsBayesianOnlineChangepoint2007,
  title = {Bayesian {{Online Changepoint Detection}}},
  author = {Adams, Ryan Prescott and MacKay, David J. C.},
  date = {2007-10-19},
  eprint = {0710.3742},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.0710.3742},
  url = {http://arxiv.org/abs/0710.3742},
  urldate = {2023-03-17},
  abstract = {Changepoints are abrupt variations in the generative parameters of a data sequence. Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem. Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to a variety of types of data. We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.},
  pubstate = {preprint},
  keywords = {/unread,Statistics - Machine Learning},
  file = {/Users/andrew/Zotero/storage/B9844L4J/Adams_MacKay_2007_Bayesian Online Changepoint Detection.pdf;/Users/andrew/Zotero/storage/36XZ5FQB/0710.html}
}

@online{amatriainTransformerModelsIntroduction2023,
  title = {Transformer Models: An Introduction and Catalog},
  shorttitle = {Transformer Models},
  author = {Amatriain, Xavier},
  date = {2023-02-11},
  eprint = {2302.07730},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.07730},
  url = {http://arxiv.org/abs/2302.07730},
  urldate = {2023-02-16},
  abstract = {In the past few years we have seen the meteoric appearance of dozens of models of the Transformer family, all of which have funny, but not self-explanatory, names. The goal of this paper is to offer a somewhat comprehensive but simple catalog and classification of the most popular Transformer models. The paper also includes an introduction to the most important aspects and innovation in Transformer models.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/94ENBB72/Amatriain_2023_Transformer models.pdf;/Users/andrew/Zotero/storage/XCL2MGKM/2302.html}
}

@article{asadiRippleConceptBasedInterpretation,
  title = {Ripple: {{Concept-Based Interpretation}} for {{Raw Time Series Models}} in {{Education}}},
  author = {Asadi, Mohammad and Swamy, Vinitra and Frej, Jibril and Vignoud, Julien and Marras, Mirko and Kaser, Tanja},
  abstract = {Time series is the most prevalent form of input data for educational prediction tasks. The vast majority of research using time series data focuses on hand-crafted features, designed by experts for predictive performance and interpretability. However, extracting these features is labor-intensive for humans and computers. In this paper, we propose an approach that utilizes irregular multivariate time series modeling with graph neural networks to achieve comparable or better accuracy with raw time series clickstreams in comparison to handcrafted features. Furthermore, we extend concept activation vectors for interpretability in raw time series models. We analyze these advances in the education domain, addressing the task of early student performance prediction for downstream targeted interventions and instructional support. Our experimental analysis on 23 MOOCs with millions of combined interactions over six behavioral dimensions show that models designed with our approach can (i) beat state-of-the-art educational time series baselines with no feature extraction and (ii) provide interpretable insights for personalized interventions. Source code: https://github.com/epfl-ml4ed/ripple/.},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/E4TQA4KF/Asadi et al. - Ripple Concept-Based Interpretation for Raw Time .pdf}
}

@article{badrinathPyBKTAccessiblePython,
  title = {{{pyBKT}}: {{An Accessible Python Library}} of {{Bayesian Knowledge Tracing Models}}},
  author = {Badrinath, Anirudhan and Wang, Frederic and Pardos, Zachary},
  abstract = {Bayesian Knowledge Tracing, a model used for cognitive mastery estimation, has been a hallmark of adaptive learning research and an integral component of deployed intelligent tutoring systems (ITS). In this paper, we provide a brief history of knowledge tracing model research and introduce pyBKT, an accessible and computationally efficient library of model extensions from the literature. The library provides data generation, fitting, prediction, and cross-validation routines, as well as a simple to use data helper interface to ingest typical tutor log dataset formats. We evaluate the runtime with various dataset sizes and compare to past implementations. Additionally, we conduct sanity checks of the model using experiments with simulated data to evaluate the accuracy of its EM parameter learning and use real-world data to validate its predictions, comparing pyBKT’s supported model variants with results from the papers in which they were originally introduced. The library is open source and open license for the purpose of making knowledge tracing more accessible to communities of research and practice and to facilitate progress in the field through easier replication of past approaches.},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/EX6WHJBD/Badrinath et al. - pyBKT An Accessible Python Library of Bayesian Kn.pdf}
}

@article{binzUsingCognitivePsychology2023,
  title = {Using Cognitive Psychology to Understand {{GPT-3}}},
  author = {Binz, Marcel and Schulz, Eric},
  date = {2023-02-07},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {6},
  pages = {e2218523120},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.2218523120},
  url = {https://www.pnas.org/doi/10.1073/pnas.2218523120},
  urldate = {2023-02-11},
  abstract = {We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3’s decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3’s behavior is impressive: It solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multiarmed bandit task, and shows signatures of model-based reinforcement learning. Yet, we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. Taken together, these results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/2DE7KRJ8/Binz_Schulz_2023_Using cognitive psychology to understand GPT-3.pdf}
}

@article{binzUsingCognitivePsychology2023a,
  title = {Using Cognitive Psychology to Understand {{GPT-3}}},
  author = {Binz, Marcel and Schulz, Eric},
  date = {2023-02-07},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {6},
  pages = {e2218523120},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.2218523120},
  url = {https://www.pnas.org/doi/10.1073/pnas.2218523120},
  urldate = {2023-03-25},
  abstract = {We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3’s decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3’s behavior is impressive: It solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multiarmed bandit task, and shows signatures of model-based reinforcement learning. Yet, we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. Taken together, these results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/J823HK9W/Binz_Schulz_2023_Using cognitive psychology to understand GPT-3.pdf}
}

@article{bozkurta.SpeculativeFuturesChatGPT2023,
  title = {Speculative Futures on {{ChatGPT}} and Generative Artificial Intelligence ({{AI}}): {{A}} Collective Reflection from the Educational Landscape},
  shorttitle = {Speculative Futures on {{ChatGPT}} and Generative Artificial Intelligence ({{AI}})},
  author = {Bozkurt, A. and Xiao, J. and Lambert, S. and Pazurek, A. and Crompton, H. and Koseoglu, S. and Farrow, R. and Bond, M. and Nerantzi, C. and Honeychurch, S. and Bali, M. and Dron, J. and Mir, K. and Stewart, B. and Costello, E. and Mason, J. and Stracke, C. M. and Romero-Hall, E. and Koutropoulos, A. and Toquero, C. M. and Singh, L. and Tlili, A. and Lee, K. and Nichols, M. and Ossiannilsson, E. and Brown, M. and Irvine, V. and Raffaghelli, J. E. and Santos-Hermosa, G. and Farrell, O. and Adam, T. and Thong, Y. L. and Sani-Bozkurt, S. and Sharma, R. C. and Hrastinski, S. and Jandrić, P.},
  date = {2023-02-13},
  publisher = {{Zenodo}},
  doi = {10.5281/ZENODO.7636568},
  url = {https://zenodo.org/record/7636568},
  urldate = {2023-03-23},
  abstract = {While ChatGPT has recently become very popular, AI has a long history and philosophy. This paper intends to explore the promises and pitfalls of the Generative Pre-trained Transformer (GPT) AI and potentially future technologies by adopting a speculative methodology. Speculative future narratives with a specific focus on educational contexts are provided in an attempt to identify emerging themes and discuss their implications for education in the 21st century. Affordances of (using) AI in Education (AIEd) and possible adverse effects are identified and discussed which emerge from the narratives. It is argued that now is the best of times to define human vs AI contribution to education because AI can accomplish more and more educational activities that used to be the prerogative of human educators. Therefore, it is imperative to rethink the respective roles of technology and human educators in education with a future-oriented mindset.},
  langid = {english},
  keywords = {/unread,artificial intelligence (AI),artificial intelligence in education (AIEd),future educational perspectives,generative pre-trained transformer (GPT),natural language processing,speculative methodology},
  file = {/Users/andrew/Zotero/storage/B7H35EFB/Bozkurt, A. et al. - 2023 - Speculative futures on ChatGPT and generative arti.pdf}
}

@online{broschinskiGrafikenErklaertFunktioniert2023,
  title = {In 9 Grafiken erklärt – So funktioniert künstliche Intelligenz},
  author = {Broschinski, Sebastian and Plattner, Titus and Meier, Patrick and Vögeli, Patrick},
  date = {2023-06-10},
  url = {https://www.derbund.ch/so-funktioniert-kuenstliche-intelligenz-599276436215},
  urldate = {2023-06-13},
  abstract = {Kann künstliche Intelligenz mehr als Äpfel und Birnen sortieren? Und warum lassen sich Computer immer noch leicht übertölpeln? Hier finden Sie alle Antworten.},
  langid = {ngerman},
  organization = {{Der Bund}},
  file = {/Users/andrew/Zotero/storage/3G74I43I/so-funktioniert-kuenstliche-intelligenz-599276436215.html}
}

@online{brownLanguageModelsAre2020a,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2023-03-01},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/SUCZVXXP/Brown et al_2020_Language Models are Few-Shot Learners.pdf;/Users/andrew/Zotero/storage/JW9ITXGI/2005.html}
}

@online{brownLanguageModelsAre2020b,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.14165},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2023-06-08},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/HSTN9QGG/Brown et al_2020_Language Models are Few-Shot Learners.pdf;/Users/andrew/Zotero/storage/VHPJRB6M/2005.html}
}

@online{bubeckSparksArtificialGeneral2023,
  title = {Sparks of {{Artificial General Intelligence}}: {{Early}} Experiments with {{GPT-4}}},
  shorttitle = {Sparks of {{Artificial General Intelligence}}},
  author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  date = {2023-04-13},
  eprint = {2303.12712},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.12712},
  urldate = {2023-05-29},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4 [Ope23], was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT4 is part of a new cohort of LLMs (along with ChatGPT and Google’s PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4’s performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4’s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/2QTBXBKP/Bubeck et al. - 2023 - Sparks of Artificial General Intelligence Early e.pdf}
}

@article{corbettKnowledgeTracingModeling1994,
  title = {Knowledge Tracing: {{Modeling}} the Acquisition of Procedural Knowledge},
  shorttitle = {Knowledge Tracing},
  author = {Corbett, Albert T. and Anderson, John R.},
  date = {1994-12-01},
  journaltitle = {User Modeling and User-Adapted Interaction},
  shortjournal = {User Model User-Adap Inter},
  volume = {4},
  number = {4},
  pages = {253--278},
  issn = {1573-1391},
  doi = {10.1007/BF01099821},
  url = {https://doi.org/10.1007/BF01099821},
  urldate = {2023-03-31},
  abstract = {This paper describes an effort to model students' changing knowledge state during skill acquisition. Students in this research are learning to write short programs with the ACT Programming Tutor (APT). APT is constructed around a production rule cognitive model of programming knowledge, called theideal student model. This model allows the tutor to solve exercises along with the student and provide assistance as necessary. As the student works, the tutor also maintains an estimate of the probability that the student has learned each of the rules in the ideal model, in a process calledknowledge tracing. The tutor presents an individualized sequence of exercises to the student based on these probability estimates until the student has ‘mastered’ each rule. The programming tutor, cognitive model and learning and performance assumptions are described. A series of studies is reviewed that examine the empirical validity of knowledge tracing and has led to modifications in the process. Currently the model is quite successful in predicting test performance. Further modifications in the modeling process are discussed that may improve performance levels.},
  langid = {english},
  keywords = {/unread,empirical validity,individual differences,intelligent tutoring systems,learning,mastery learning,procedural knowledge,Student modeling},
  file = {/Users/andrew/Zotero/storage/BCTFGDNV/Corbett_Anderson_1994_Knowledge tracing.pdf}
}

@online{creswellFaithfulReasoningUsing2022,
  title = {Faithful {{Reasoning Using Large Language Models}}},
  author = {Creswell, Antonia and Shanahan, Murray},
  date = {2022-08-30},
  eprint = {2208.14271},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2208.14271},
  url = {http://arxiv.org/abs/2208.14271},
  urldate = {2023-04-21},
  abstract = {Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/JHWPXKW7/Creswell_Shanahan_2022_Faithful Reasoning Using Large Language Models.pdf;/Users/andrew/Zotero/storage/C9TB3MXG/2208.html}
}

@online{daiCanLargeLanguage2023,
  title = {Can {{Large Language Models Provide Feedback}} to {{Students}}? {{A Case Study}} on {{ChatGPT}}},
  shorttitle = {Can {{Large Language Models Provide Feedback}} to {{Students}}?},
  author = {Dai, Wei and Lin, Jionghao and Jin, Flora and Li, Tongguang and Tsai, Yi-Shan and Gasevic, Dragan and Chen, Guanliang},
  date = {2023-04-13T06:49:18},
  doi = {10.35542/osf.io/hcgzj},
  url = {https://edarxiv.org/hcgzj/},
  urldate = {2023-04-15},
  abstract = {Educational feedback has been widely acknowledged as an effective approach to improving student learning. However, scaling effective practices can be laborious and costly, which motivated researchers to work on automated feedback systems (AFS). Inspired by the recent advancements in the pre-trained language models (e.g., ChatGPT), we posit that such models might advance the existing knowledge of textual feedback generation in AFS because of their capability to offer natural-sounding and detailed responses. Therefore, we aimed to investigate the feasibility of using ChatGPT to provide students with feedback to help them learn better. Specifically, we first examined the readability of ChatGPT-generated feedback. Then, we measured the agreement between ChatGPT and the instructor when assessing students' assignments according to the marking rubric. Finally, we used a well-known theoretical feedback framework to further investigate the effectiveness of the feedback generated by ChatGPT. Our results show that i) ChatGPT is capable of generating more detailed feedback that fluently and coherently summarizes students' performance than human instructors; ii) ChatGPT achieved high agreement with the instructor when assessing the topic of students' assignments; and iii) ChatGPT could provide feedback on the process of students completing the task, which benefits students developing learning skills.},
  langid = {american},
  pubstate = {preprint},
  keywords = {and Research,Automated Feedback,Education,Educational Assessment,Educational Methods,Evaluation,Feedback Effectiveness,Feedback Generation,Higher Education,Large Language Model},
  file = {/Users/andrew/Zotero/storage/ICYKBJ87/Dai et al_2023_Can Large Language Models Provide Feedback to Students.pdf}
}

@article{degallier-rochatHumanAugmentationNot2022,
  title = {Human Augmentation, Not Replacement: {{A}} Research Agenda for {{AI}} and Robotics in the Industry},
  shorttitle = {Human Augmentation, Not Replacement},
  author = {Dégallier-Rochat, Sarah and Kurpicz-Briki, Mascha and Endrissat, Nada and Yatsenko, Olena},
  date = {2022},
  journaltitle = {Frontiers in Robotics and AI},
  volume = {9},
  issn = {2296-9144},
  doi = {10.3389/frobt.2022.997386},
  url = {https://www.frontiersin.org/articles/10.3389/frobt.2022.997386},
  urldate = {2023-03-03},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/9QEXUZ9N/Dégallier-Rochat et al_2022_Human augmentation, not replacement.pdf}
}

@article{desaireDistinguishingAcademicScience2023,
  title = {Distinguishing Academic Science Writing from Humans or {{ChatGPT}} with over 99\% Accuracy Using Off-the-Shelf Machine Learning Tools},
  author = {Desaire, Heather and Chua, Aleesa E. and Isom, Madeline and Jarosova, Romana and Hua, David},
  date = {2023-06},
  journaltitle = {Cell Reports Physical Science},
  shortjournal = {Cell Reports Physical Science},
  pages = {101426},
  issn = {26663864},
  doi = {10.1016/j.xcrp.2023.101426},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S266638642300200X},
  urldate = {2023-06-08},
  abstract = {ChatGPT has enabled access to artificial intelligence (AI)-generated writing for the masses, initiating a culture shift in the way people work, learn, and write. The need to discriminate human writing from AI is now both critical and urgent. Addressing this need, we report a method for discriminating text generated by ChatGPT from (human) academic scientists, relying on prevalent and accessible supervised classification methods. The approach uses new features for discriminating (these) humans from AI; as examples, scientists write long paragraphs and have a penchant for equivocal language, frequently using words like ‘‘but,’’ ‘‘however,’’ and ‘‘although.’’ With a set of 20 features, we built a model that assigns the author, as human or AI, at over 99\% accuracy. This strategy could be further adapted and developed by others with basic skills in supervised classification, enabling access to many highly accurate and targeted models for detecting AI usage in academic writing and beyond.},
  langid = {english},
  file = {/Users/andrew/Zotero/storage/VFDLC92L/Desaire et al. - 2023 - Distinguishing academic science writing from human.pdf}
}

@article{desaireDistinguishingAcademicScience2023a,
  title = {Distinguishing Academic Science Writing from Humans or {{ChatGPT}} with over 99\% Accuracy Using Off-the-Shelf Machine Learning Tools},
  author = {Desaire, Heather and Chua, Aleesa E. and Isom, Madeline and Jarosova, Romana and Hua, David},
  date = {2023-06-07},
  journaltitle = {Cell Reports Physical Science},
  shortjournal = {Cell Reports Physical Science},
  pages = {101426},
  issn = {2666-3864},
  doi = {10.1016/j.xcrp.2023.101426},
  url = {https://www.sciencedirect.com/science/article/pii/S266638642300200X},
  urldate = {2023-06-11},
  abstract = {ChatGPT has enabled access to artificial intelligence (AI)-generated writing for the masses, initiating a culture shift in the way people work, learn, and write. The need to discriminate human writing from AI is now both critical and urgent. Addressing this need, we report a method for discriminating text generated by ChatGPT from (human) academic scientists, relying on prevalent and accessible supervised classification methods. The approach uses new features for discriminating (these) humans from AI; as examples, scientists write long paragraphs and have a penchant for equivocal language, frequently using words like “but,” “however,” and “although.” With a set of 20 features, we built a model that assigns the author, as human or AI, at over 99\% accuracy. This strategy could be further adapted and developed by others with basic skills in supervised classification, enabling access to many highly accurate and targeted models for detecting AI usage in academic writing and beyond.},
  langid = {english},
  keywords = {AI,ChatGPT,machine learing,plagiarism,text analysis,XGBoost},
  file = {/Users/andrew/Zotero/storage/QT575F6U/Desaire et al_2023_Distinguishing academic science writing from humans or ChatGPT with over 99%.pdf;/Users/andrew/Zotero/storage/Q3KICH9D/S266638642300200X.html}
}

@article{DidaktischeUndRechtliche2023,
  title = {Didaktische Und Rechtliche {{Perspektiven}} Auf {{KI-gestütztes Schreiben}} in Der {{Hochschulbildung}}},
  date = {2023-03-07},
  url = {https://hss-opus.ub.ruhr-uni-bochum.de/opus4/frontdoor/index/index/docId/9734},
  urldate = {2023-03-15},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/4IZVMT7F/2023_Didaktische und rechtliche Perspektiven auf KI-gestütztes Schreiben in der.pdf}
}

@online{dohanLanguageModelCascades2022,
  title = {Language {{Model Cascades}}},
  author = {Dohan, David and Xu, Winnie and Lewkowycz, Aitor and Austin, Jacob and Bieber, David and Lopes, Raphael Gontijo and Wu, Yuhuai and Michalewski, Henryk and Saurous, Rif A. and Sohl-dickstein, Jascha and Murphy, Kevin and Sutton, Charles},
  date = {2022-07-28},
  eprint = {2207.10342},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.10342},
  url = {http://arxiv.org/abs/2207.10342},
  urldate = {2023-04-21},
  abstract = {Prompted models have demonstrated impressive few-shot learning abilities. Repeated interactions at test-time with a single model, or the composition of multiple models together, further expands capabilities. These compositions are probabilistic models, and may be expressed in the language of graphical models with random variables whose values are complex data types such as strings. Cases with control flow and dynamic structure require techniques from probabilistic programming, which allow implementing disparate model structures and inference strategies in a unified language. We formalize several existing techniques from this perspective, including scratchpads / chain of thought, verifiers, STaR, selection-inference, and tool use. We refer to the resulting programs as language model cascades.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/V8KLV6ZR/Dohan et al_2022_Language Model Cascades.pdf;/Users/andrew/Zotero/storage/Y74R2PJV/2207.html}
}

@article{ghoshOptionTracingBinary,
  title = {Option {{Tracing}}: {{Beyond Binary Knowledge Tracing}}},
  author = {Ghosh, Aritra and Lan, Andrew S},
  abstract = {This paper details our solutions to Tasks 1\&2 of the NeurIPS 2020 Education Challenge.1 Knowledge tracing, a family of methods to estimate each student’s mastery levels on skills/knowledge components from their past responses to assessment questions, is useful for progress monitoring, personalization, and helping teachers to deliver personalized and targeted feedback to students to improve their learning outcomes. One key limitation of current knowledge tracing methods is that they can only estimate an overall knowledge level of a student since they analyze only the binary-valued correctness of student responses. We adapt a series of popular knowledge tracing methods to the task of option prediction in multiple choice questions. Experimental results show that our method performs well on both option prediction and correctness prediction.},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/IHK5ULH6/Ghosh and Lan - Option Tracing Beyond Binary Knowledge Tracing.pdf}
}

@online{hagendorffMachinePsychologyInvestigating2023,
  title = {Machine {{Psychology}}: {{Investigating Emergent Capabilities}} and {{Behavior}} in {{Large Language Models Using Psychological Methods}}},
  shorttitle = {Machine {{Psychology}}},
  author = {Hagendorff, Thilo},
  date = {2023-04-11},
  eprint = {2303.13988},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.13988},
  url = {http://arxiv.org/abs/2303.13988},
  urldate = {2023-04-20},
  abstract = {Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called "machine psychology". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behavioral patterns discovered in LLMs are to be interpreted. In sum, machine psychology aims to discover emergent abilities in LLMs that cannot be detected by most traditional natural language processing benchmarks.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/GGMKIIXK/Hagendorff_2023_Machine Psychology.pdf;/Users/andrew/Zotero/storage/GNLYS9D7/2303.html}
}

@online{hrefHowDoesIncontext,
  title = {How Does In-Context Learning Work? {{A}} Framework for Understanding the Differences from Traditional Supervised Learning},
  shorttitle = {How Does In-Context Learning Work?},
  author = {{href=},},
  url = {http://ai.stanford.edu/blog/understanding-incontext/},
  urldate = {2023-03-28},
  abstract = {The official Stanford AI Lab blog},
  keywords = {/unread}
}

@online{huangLanguageNotAll2023,
  title = {Language {{Is Not All You Need}}: {{Aligning Perception}} with {{Language Models}}},
  shorttitle = {Language {{Is Not All You Need}}},
  author = {Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Patra, Barun and Liu, Qiang and Aggarwal, Kriti and Chi, Zewen and Bjorck, Johan and Chaudhary, Vishrav and Som, Subhojit and Song, Xia and Wei, Furu},
  date = {2023-03-01},
  eprint = {2302.14045},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.14045},
  urldate = {2023-03-02},
  abstract = {A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/andrew/Zotero/storage/U6D9JARC/Huang et al_2023_Language Is Not All You Need.pdf;/Users/andrew/Zotero/storage/MEX5ATVT/2302.html}
}

@online{huangReasoningLargeLanguage2022,
  title = {Towards {{Reasoning}} in {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Towards {{Reasoning}} in {{Large Language Models}}},
  author = {Huang, Jie and Chang, Kevin Chen-Chuan},
  date = {2022-12-20},
  eprint = {2212.10403},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.10403},
  url = {http://arxiv.org/abs/2212.10403},
  urldate = {2023-04-20},
  abstract = {Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/XL6CTLJE/Huang_Chang_2022_Towards Reasoning in Large Language Models.pdf;/Users/andrew/Zotero/storage/D4H273EY/2212.html}
}

@inproceedings{hunzikerTeachingMultipleConcepts2019a,
  title = {Teaching {{Multiple Concepts}} to a {{Forgetful Learner}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hunziker, Anette and Chen, Yuxin and Mac Aodha, Oisin and Gomez Rodriguez, Manuel and Krause, Andreas and Perona, Pietro and Yue, Yisong and Singla, Adish},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/2952351097998ac1240cb2ab7333a3d2-Abstract.html},
  urldate = {2023-02-13},
  abstract = {How can we help a forgetful learner learn multiple concepts within a limited time frame? While there have been extensive studies in designing optimal schedules for teaching a single concept given a learner's memory model, existing approaches for teaching multiple concepts are typically based on heuristic scheduling techniques without theoretical guarantees. In this paper, we look at the problem from the perspective of discrete optimization and introduce a novel algorithmic framework for teaching multiple concepts with strong performance guarantees.  Our framework is both generic, allowing the design of teaching schedules for different memory models, and also interactive, allowing the teacher to adapt the schedule to the underlying forgetting mechanisms of the learner. Furthermore, for a well-known memory model, we are able to identify a regime of model parameters where our framework is guaranteed to achieve high performance. We perform extensive evaluations using simulations along with real user studies in two concrete applications: (i) an educational app for online vocabulary teaching; and (ii) an app for teaching novices how to recognize animal species from images.  Our results demonstrate the effectiveness of our algorithm compared to popular heuristic approaches.},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/W2EDAAKP/Hunziker et al_2019_Teaching Multiple Concepts to a Forgetful Learner.pdf}
}

@online{huThoughtCloningLearning2023,
  title = {Thought {{Cloning}}: {{Learning}} to {{Think}} While {{Acting}} by {{Imitating Human Thinking}}},
  shorttitle = {Thought {{Cloning}}},
  author = {Hu, Shengran and Clune, Jeff},
  date = {2023-05-31},
  eprint = {2306.00323},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.00323},
  url = {http://arxiv.org/abs/2306.00323},
  urldate = {2023-06-09},
  abstract = {Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to think like humans do. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, but also the thoughts humans have as they perform these behaviors. While we expect Thought Cloning to truly shine at scale on internet-sized datasets of humans thinking out loud while acting (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than Behavioral Cloning and its performance advantage grows the further out of distribution test tasks are, highlighting its ability to better handle novel situations. Thought Cloning also provides important benefits for AI Safety and Interpretability, and makes it easier to debug and improve AI. Because we can observe the agent's thoughts, we can (1) more easily diagnose why things are going wrong, making it easier to fix the problem, (2) steer the agent by correcting its thinking, or (3) prevent it from doing unsafe things it plans to do. Overall, by training agents how to think as well as behave, Thought Cloning creates safer, more powerful agents.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/XGKRWFQ6/Hu_Clune_2023_Thought Cloning.pdf;/Users/andrew/Zotero/storage/7DUCYMMG/2306.html}
}

@online{imaniMathPrompterMathematicalReasoning2023,
  title = {{{MathPrompter}}: {{Mathematical Reasoning}} Using {{Large Language Models}}},
  shorttitle = {{{MathPrompter}}},
  author = {Imani, Shima and Du, Liang and Shrivastava, Harsh},
  date = {2023-03-03},
  eprint = {2303.05398},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.05398},
  urldate = {2023-03-23},
  abstract = {Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose `MathPrompter', a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple Algebraic expressions or Python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the MultiArith dataset (\$78.7\textbackslash\%\textbackslash rightarrow92.5\textbackslash\%\$) evaluated using 175B parameter GPT-based LLM.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/7A7HVRIY/Imani et al_2023_MathPrompter.pdf;/Users/andrew/Zotero/storage/PGSEZPZP/2303.html}
}

@incollection{kaserKnowledgeTracingModeling2014,
  title = {Beyond {{Knowledge Tracing}}: {{Modeling Skill Topologies}} with {{Bayesian Networks}}},
  shorttitle = {Beyond {{Knowledge Tracing}}},
  booktitle = {Intelligent {{Tutoring Systems}}},
  author = {Käser, Tanja and Klingler, Severin and Schwing, Alexander Gerhard and Gross, Markus},
  editor = {Trausan-Matu, Stefan and Boyer, Kristy Elizabeth and Crosby, Martha and Panourgia, Kitty},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard},
  editorbtype = {redactor},
  date = {2014},
  volume = {8474},
  pages = {188--198},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-07221-0_23},
  url = {http://link.springer.com/10.1007/978-3-319-07221-0_23},
  urldate = {2023-02-23},
  abstract = {Modeling and predicting student knowledge is a fundamental task of an intelligent tutoring system. A popular approach for student modeling is Bayesian Knowledge Tracing (BKT). BKT models, however, lack the ability to describe the hierarchy and relationships between the different skills of a learning domain. In this work, we therefore aim at increasing the representational power of the student model by employing dynamic Bayesian networks that are able to represent such skill topologies. To ensure model interpretability, we constrain the parameter space. We evaluate the performance of our models on five large-scale data sets of different learning domains such as mathematics, spelling learning and physics, and demonstrate that our approach outperforms BKT in prediction accuracy on unseen data across all learning domains.},
  isbn = {978-3-319-07220-3 978-3-319-07221-0},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/PVTWQBMU/Käser et al. - 2014 - Beyond Knowledge Tracing Modeling Skill Topologie.pdf}
}

@incollection{kaserKnowledgeTracingModeling2014a,
  title = {Beyond {{Knowledge Tracing}}: {{Modeling Skill Topologies}} with {{Bayesian Networks}}},
  shorttitle = {Beyond {{Knowledge Tracing}}},
  booktitle = {Intelligent {{Tutoring Systems}}},
  author = {Käser, Tanja and Klingler, Severin and Schwing, Alexander Gerhard and Gross, Markus},
  editor = {Trausan-Matu, Stefan and Boyer, Kristy Elizabeth and Crosby, Martha and Panourgia, Kitty},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard},
  editorbtype = {redactor},
  date = {2014},
  volume = {8474},
  pages = {188--198},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-07221-0_23},
  url = {http://link.springer.com/10.1007/978-3-319-07221-0_23},
  urldate = {2023-03-08},
  abstract = {Modeling and predicting student knowledge is a fundamental task of an intelligent tutoring system. A popular approach for student modeling is Bayesian Knowledge Tracing (BKT). BKT models, however, lack the ability to describe the hierarchy and relationships between the different skills of a learning domain. In this work, we therefore aim at increasing the representational power of the student model by employing dynamic Bayesian networks that are able to represent such skill topologies. To ensure model interpretability, we constrain the parameter space. We evaluate the performance of our models on five large-scale data sets of different learning domains such as mathematics, spelling learning and physics, and demonstrate that our approach outperforms BKT in prediction accuracy on unseen data across all learning domains.},
  isbn = {978-3-319-07220-3 978-3-319-07221-0},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/96L8MSFW/Käser et al. - 2014 - Beyond Knowledge Tracing Modeling Skill Topologie.pdf}
}

@online{kirchenbauerWatermarkLargeLanguage2023,
  title = {A {{Watermark}} for {{Large Language Models}}},
  author = {Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  date = {2023-01-27},
  eprint = {2301.10226},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.10226},
  url = {http://arxiv.org/abs/2301.10226},
  urldate = {2023-02-15},
  abstract = {Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/QIQVSX4J/Kirchenbauer et al_2023_A Watermark for Large Language Models.pdf;/Users/andrew/Zotero/storage/IUF8M4VC/2301.html}
}

@online{kosinskiTheoryMindMay2023,
  title = {Theory of {{Mind May Have Spontaneously Emerged}} in {{Large Language Models}}},
  author = {Kosinski, Michal},
  date = {2023-03-14},
  eprint = {2302.02083},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.02083},
  url = {http://arxiv.org/abs/2302.02083},
  urldate = {2023-04-20},
  abstract = {Theory of mind (ToM), or the ability to impute unobservable mental states to others, is central to human social interactions, communication, empathy, self-consciousness, and morality. We tested several language models using 40 classic false-belief tasks widely used to test ToM in humans. The models published before 2020 showed virtually no ability to solve ToM tasks. Yet, the first version of GPT-3 ("davinci-001"), published in May 2020, solved about 40\% of false-belief tasks-performance comparable with 3.5-year-old children. Its second version ("davinci-002"; January 2022) solved 70\% of false-belief tasks, performance comparable with six-year-olds. Its most recent version, GPT-3.5 ("davinci-003"; November 2022), solved 90\% of false-belief tasks, at the level of seven-year-olds. GPT-4 published in March 2023 solved nearly all the tasks (95\%). These findings suggest that ToM-like ability (thus far considered to be uniquely human) may have spontaneously emerged as a byproduct of language models' improving language skills.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction},
  file = {/Users/andrew/Zotero/storage/PSVQIFRD/Kosinski_2023_Theory of Mind May Have Spontaneously Emerged in Large Language Models.pdf;/Users/andrew/Zotero/storage/IGGWBMA9/2302.html}
}

@online{kungPerformanceChatGPTUSMLE2022,
  title = {Performance of {{ChatGPT}} on {{USMLE}}: {{Potential}} for {{AI-Assisted Medical Education Using Large Language Models}}},
  shorttitle = {Performance of {{ChatGPT}} on {{USMLE}}},
  author = {Kung, Tiffany H. and Cheatham, Morgan and ChatGPT and Medenilla, Arielle and Sillos, Czarina and Leon, Lorie De and Elepaño, Camille and Madriaga, Maria and Aggabao, Rimel and Diaz-Candido, Giezel and Maningo, James and Tseng, Victor},
  date = {2022-12-21},
  eprinttype = {medRxiv},
  pages = {2022.12.19.22283643},
  doi = {10.1101/2022.12.19.22283643},
  url = {https://www.medrxiv.org/content/10.1101/2022.12.19.22283643v2},
  urldate = {2023-02-11},
  abstract = {We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making.},
  langid = {english},
  pubstate = {preprint},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/5SATZSAZ/Kung et al_2022_Performance of ChatGPT on USMLE.pdf}
}

@article{kurpicz-brikiCulturalDifferencesBias,
  title = {Cultural {{Differences}} in {{Bias}}? {{Origin}} and {{Gender Bias}} in {{Pre-Trained German}} and {{French Word Embeddings}}},
  author = {Kurpicz-Briki, Mascha},
  abstract = {Smart applications often rely on training data in form of text. If there is a bias in that training data, the decision of the applications might not be fair. Common training data has been shown to be biased towards different groups of minorities. However, there is no generic algorithm to determine the fairness of training data. One existing approach is to measure gender bias using word embeddings. Most research in this field has been dedicated to the English language. In this work, we identified that there is a bias towards gender and origin in both German and French word embeddings. In particular, we found that real-world bias and stereotypes from the 18th century are still included in today’s word embeddings. Furthermore, we show that the gender bias in German has a different form from English and there is indication that bias has cultural differences that need to be considered when analyzing texts and word embeddings in different languages.},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/X4NJQJNZ/Kurpicz-Briki - Cultural Differences in Bias Origin and Gender Bi.pdf}
}

@article{kurpicz-brikiWorldFullStereotypes2021,
  title = {A {{World Full}} of {{Stereotypes}}? {{Further Investigation}} on {{Origin}} and {{Gender Bias}} in {{Multi-Lingual Word Embeddings}}},
  shorttitle = {A {{World Full}} of {{Stereotypes}}?},
  author = {Kurpicz-Briki, Mascha and Leoni, Tomaso},
  date = {2021},
  journaltitle = {Frontiers in Big Data},
  volume = {4},
  issn = {2624-909X},
  doi = {10.3389/fdata.2021.625290},
  url = {https://www.frontiersin.org/articles/10.3389/fdata.2021.625290},
  urldate = {2023-03-03},
  abstract = {Publicly available off-the-shelf word embeddings that are often used in productive applications for natural language processing have been proven to be biased. We have previously shown that this bias can come in different forms, depending on the language and the cultural context. In this work, we extend our previous work and further investigate how bias varies in different languages. We examine Italian and Swedish word embeddings for gender and origin bias, and demonstrate how an origin bias concerning local migration groups in Switzerland is included in German word embeddings. We propose BiasWords, a method to automatically detect new forms of bias. Finally, we discuss how cultural and language aspects are relevant to the impact of bias on the application and to potential mitigation measures.},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/6GGF3AFM/Kurpicz-Briki_Leoni_2021_A World Full of Stereotypes.pdf}
}

@online{kwonRewardDesignLanguage2023,
  title = {Reward {{Design}} with {{Language Models}}},
  author = {Kwon, Minae and Xie, Sang Michael and Bullard, Kalesha and Sadigh, Dorsa},
  date = {2023-02-27},
  eprint = {2303.00001},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.00001},
  urldate = {2023-03-10},
  abstract = {Reward design in reinforcement learning (RL) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. Can we instead cheaply design rewards using a natural language interface? This paper explores how to simplify reward design by prompting a large language model (LLM) such as GPT-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of the desired behavior. Our approach leverages this proxy reward function in an RL framework. Specifically, users specify a prompt once at the beginning of training. During training, the LLM evaluates an RL agent's behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. The RL agent then uses this reward to update its behavior. We evaluate whether our approach can train agents aligned with user objectives in the Ultimatum Game, matrix games, and the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents trained with our framework are well-aligned with the user's objectives and outperform RL agents trained with reward functions learned via supervised learning},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/AIXZC8BS/Kwon et al_2023_Reward Design with Language Models.pdf;/Users/andrew/Zotero/storage/PI3A34ML/2303.html}
}

@article{lakeBuildingMachinesThat2017,
  title = {Building Machines That Learn and Think like People},
  author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
  year = {2017/ed},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {40},
  publisher = {{Cambridge University Press}},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X16001837},
  url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-like-people/A9535B1D745A0377E16C590E14B94993#},
  urldate = {2021-02-08},
  abstract = {Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
  langid = {english},
  file = {/Users/andrew/Zotero/storage/2TZWZ4KG/Lake et al_2017_Building machines that learn and think like people.pdf;/Users/andrew/Zotero/storage/2LBPILKD/A9535B1D745A0377E16C590E14B94993.html}
}

@article{lakeHumanFewshotLearning,
  title = {Human Few-Shot Learning of Compositional Instructions},
  author = {Lake, Brenden M and Linzen, Tal and Baroni, Marco},
  abstract = {People learn in fast and flexible ways that have not been emulated by machines. Once a person learns a new verb “dax,” he or she can effortlessly understand how to “dax twice,” “walk and dax,” or “dax vigorously.” There have been striking recent improvements in machine learning for natural language processing, yet the best algorithms require vast amounts of experience and struggle to generalize new concepts in compositional ways. To better understand these distinctively human abilities, we study the compositional skills of people through languagelike instruction learning tasks. Our results show that people can learn and use novel functional concepts from very few examples (few-shot learning), successfully applying familiar functions to novel inputs. People can also compose concepts in complex ways that go beyond the provided demonstrations. Two additional experiments examined the assumptions and inductive biases that people make when solving these tasks, revealing three biases: mutual exclusivity, one-to-one mappings, and iconic concatenation. We discuss the implications for cognitive modeling and the potential for building machines with more human-like language learning capabilities.},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/W8H4AGKW/Lake et al. - Human few-shot learning of compositional instructi.pdf}
}

@article{lakePeopleInferRecursive2020,
  title = {People {{Infer Recursive Visual Concepts}} from {{Just}} a {{Few Examples}}},
  author = {Lake, Brenden M. and Piantadosi, Steven T.},
  date = {2020-03-01},
  journaltitle = {Computational Brain \& Behavior},
  shortjournal = {Comput Brain Behav},
  volume = {3},
  number = {1},
  pages = {54--65},
  issn = {2522-087X},
  doi = {10.1007/s42113-019-00053-y},
  url = {https://doi.org/10.1007/s42113-019-00053-y},
  urldate = {2022-02-03},
  abstract = {Machine learning has made major advances in categorizing objects in images, yet the best algorithms miss important aspects of how people learn and think about categories. People can learn richer concepts from fewer examples, including causal models that explain how members of a category are formed. Here, we explore the limits of this human ability to infer causal “programs”—latent generating processes with nontrivial algorithmic properties—from one, two, or three visual examples. People were asked to extrapolate the programs in several ways, for both classifying and generating new examples. As a theory of these inductive abilities, we present a Bayesian program learning model that searches the space of programs for the best explanation of the observations. Although variable, people’s judgments are broadly consistent with the model and inconsistent with several alternatives, including a pretrained deep neural network for object recognition, indicating that people can learn and reason with rich algorithmic abstractions from sparse input data.},
  langid = {english},
  file = {/Users/andrew/Zotero/storage/GMQQPPDV/Lake and Piantadosi - 2020 - People Infer Recursive Visual Concepts from Just a.pdf}
}

@online{lakeWordMeaningMinds2021,
  title = {Word Meaning in Minds and Machines},
  author = {Lake, Brenden M. and Murphy, Gregory L.},
  date = {2021-04-17},
  eprint = {2008.01766},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2008.01766},
  url = {http://arxiv.org/abs/2008.01766},
  urldate = {2023-02-11},
  abstract = {Machines have achieved a broad and growing set of linguistic competencies, thanks to recent progress in Natural Language Processing (NLP). Psychologists have shown increasing interest in such models, comparing their output to psychological judgments such as similarity, association, priming, and comprehension, raising the question of whether the models could serve as psychological theories. In this article, we compare how humans and machines represent the meaning of words. We argue that contemporary NLP systems are fairly successful models of human word similarity, but they fall short in many other respects. Current models are too strongly linked to the text-based patterns in large corpora, and too weakly linked to the desires, goals, and beliefs that people express through words. Word meanings must also be grounded in perception and action and be capable of flexible combinations in ways that current systems are not. We discuss more promising approaches to grounding NLP systems and argue that they will be more successful with a more human-like, conceptual basis for word meaning.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/UM2SEPQH/Lake_Murphy_2021_Word meaning in minds and machines.pdf;/Users/andrew/Zotero/storage/R7MFY27P/2008.html}
}

@article{lakeWordMeaningMinds2021a,
  title = {Word Meaning in Minds and Machines.},
  author = {Lake, Brenden M. and Murphy, Gregory L.},
  date = {2021-07-22},
  journaltitle = {Psychological Review},
  shortjournal = {Psychological Review},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/rev0000297},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/rev0000297},
  urldate = {2023-03-04},
  abstract = {Machines have achieved a broad and growing set of linguistic competencies, thanks to recent progress in Natural Language Processing (NLP). Psychologists have shown increasing interest in such models, comparing their output to psychological judgments such as similarity, association, priming, and comprehension, raising the question of whether the models could serve as psychological theories. In this article, we compare how humans and machines represent the meaning of words. We argue that contemporary NLP systems are fairly successful models of human word similarity, but they fall short in many other respects. Current models are too strongly linked to the text-based patterns in large corpora, and too weakly linked to the desires, goals, and beliefs that people express through words. Word meanings must also be grounded in perception and action and be capable of flexible combinations in ways that current systems are not. We discuss promising approaches to grounding NLP systems and argue that they will be more successful, with a more human-like, conceptual basis for word meaning.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/QN5IJG5Q/Lake and Murphy - 2021 - Word meaning in minds and machines..pdf}
}

@online{lampinenPassiveLearningActive2023,
  title = {Passive Learning of Active Causal Strategies in Agents and Language Models},
  author = {Lampinen, Andrew Kyle and Chan, Stephanie C. Y. and Dasgupta, Ishita and Nam, Andrew J. and Wang, Jane X.},
  date = {2023-05-25},
  eprint = {2305.16183},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.16183},
  urldate = {2023-05-29},
  abstract = {What can be learned about causality and experimentation from passive data? This question is salient given recent successes of passively-trained language models in interactive domains such as tool use. Passive learning is inherently limited. However, we show that purely passive learning can in fact allow an agent to learn generalizable strategies for determining and using causal structures, as long as the agent can intervene at test time. We formally illustrate that learning a strategy of first experimenting, then seeking goals, can allow generalization from passive learning in principle. We then show empirically that agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data; these agents can also generalize experimentation strategies to novel variable sets never observed in training. We then show that strategies for causal intervention and exploitation can be generalized from passive data even in a more complex environment with high-dimensional observations, with the support of natural language explanations. Explanations can even allow passive learners to generalize out-of-distribution from perfectlyconfounded training data. Finally, we show that language models, trained only on passive next-word prediction, can generalize causal intervention strategies from a few-shot prompt containing examples of experimentation, together with explanations and reasoning. These results highlight the surprising power of passive learning of active causal strategies, and may help to understand the behaviors and capabilities of language models.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/7LP2SCKD/Lampinen et al. - 2023 - Passive learning of active causal strategies in ag.pdf}
}

@online{lehnertAIInsightsTheoretical2023,
  title = {{{AI Insights}} into {{Theoretical Physics}} and the {{Swampland Program}}: {{A Journey Through}} the {{Cosmos}} with {{ChatGPT}}},
  shorttitle = {{{AI Insights}} into {{Theoretical Physics}} and the {{Swampland Program}}},
  author = {Lehnert, Kay},
  date = {2023-01-10},
  eprint = {2301.08155},
  eprinttype = {arxiv},
  eprintclass = {physics},
  doi = {10.48550/arXiv.2301.08155},
  url = {http://arxiv.org/abs/2301.08155},
  urldate = {2023-02-15},
  abstract = {In this case study, we explore the capabilities and limitations of ChatGPT, a natural language processing model developed by OpenAI, in the field of string theoretical swampland conjectures. We find that it is effective at paraphrasing and explaining concepts in a variety of styles, but not at genuinely connecting concepts. It will provide false information with full confidence and make up statements when necessary. However, its ingenious use of language can be fruitful for identifying analogies and describing visual representations of abstract concepts.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Physics - Popular Physics},
  file = {/Users/andrew/Zotero/storage/JYCW4CH3/Lehnert_2023_AI Insights into Theoretical Physics and the Swampland Program.pdf;/Users/andrew/Zotero/storage/BD79CY89/2301.html}
}

@online{liEmergentWorldRepresentations2023,
  title = {Emergent {{World Representations}}: {{Exploring}} a {{Sequence Model Trained}} on a {{Synthetic Task}}},
  shorttitle = {Emergent {{World Representations}}},
  author = {Li, Kenneth and Hopkins, Aspen K. and Bau, David and Viégas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  date = {2023-01-25},
  eprint = {2210.13382},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.13382},
  urldate = {2023-02-15},
  abstract = {Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network and create "latent saliency maps" that can help explain predictions in human terms.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/5ZTLQG5D/Li et al_2023_Emergent World Representations.pdf;/Users/andrew/Zotero/storage/K77V6GLW/2210.html}
}

@online{liuLLMEmpoweringLarge2023,
  title = {{{LLM}}+{{P}}: {{Empowering Large Language Models}} with {{Optimal Planning Proficiency}}},
  shorttitle = {{{LLM}}+{{P}}},
  author = {Liu, Bo and Jiang, Yuqian and Zhang, Xiaohan and Liu, Qiang and Zhang, Shiqi and Biswas, Joydeep and Stone, Peter},
  date = {2023-04-22},
  eprint = {2304.11477},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.11477},
  url = {http://arxiv.org/abs/2304.11477},
  urldate = {2023-04-28},
  abstract = {Large language models (LLMs) have demonstrated remarkable zero-shot generalization abilities: state-of-the-art chatbots can provide plausible answers to many common questions that arise in daily life. However, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal, plans. In an effort to get the best of both worlds, this paper introduces LLM+P, the first framework that incorporates the strengths of classical planners into LLMs. LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution, and then translating the found solution back into natural language. Along with LLM+P, we define a diverse set of different benchmark problems taken from common planning scenarios. Via a comprehensive set of experiments on these benchmark problems, we find that LLM+P is able to provide optimal solutions for most problems, while LLMs fail to provide even feasible plans for most problems.\textbackslash footnote\{The code and results are publicly available at https://github.com/Cranial-XIX/llm-pddl.git.\vphantom\}},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/andrew/Zotero/storage/6JKMPU69/Liu et al_2023_LLM+P.pdf;/Users/andrew/Zotero/storage/HYEFMIMG/2304.html}
}

@online{liuWeReAfraid2023,
  title = {We're {{Afraid Language Models Aren}}'t {{Modeling Ambiguity}}},
  author = {Liu, Alisa and Wu, Zhaofeng and Michael, Julian and Suhr, Alane and West, Peter and Koller, Alexander and Swayamdipta, Swabha and Smith, Noah A. and Choi, Yejin},
  date = {2023-04-27},
  eprint = {2304.14399},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.14399},
  url = {http://arxiv.org/abs/2304.14399},
  urldate = {2023-04-28},
  abstract = {Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models (LMs) are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We characterize ambiguity in a sentence by its effect on entailment relations with another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for the recent GPT-4, whose generated disambiguations are considered correct only 32\% of the time in human evaluation, compared to 90\% for disambiguations in our dataset. Finally, to illustrate the value of ambiguity-sensitive tools, we show that a multilabel NLI model can flag political claims in the wild that are misleading due to ambiguity. We encourage the field to rediscover the importance of ambiguity for NLP.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/CKKEQ77I/Liu et al_2023_We're Afraid Language Models Aren't Modeling Ambiguity.pdf;/Users/andrew/Zotero/storage/442HT3FD/2304.html}
}

@article{luckinEmpoweringEducatorsBe2022,
  title = {Empowering Educators to Be {{AI-ready}}},
  author = {Luckin, Rosemary and Cukurova, Mutlu and Kent, Carmel and family=Boulay, given=Benedict, prefix=du, useprefix=true},
  date = {2022-01-01},
  journaltitle = {Computers and Education: Artificial Intelligence},
  shortjournal = {Computers and Education: Artificial Intelligence},
  volume = {3},
  pages = {100076},
  issn = {2666-920X},
  doi = {10.1016/j.caeai.2022.100076},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000315},
  urldate = {2023-04-26},
  abstract = {In this paper, we present the concept of AI Readiness, along with a framework for developing AI Readiness training. ‘AI Readiness’ can be framed as a contextualised way of helping people to understand AI, in particular, data-driven AI. The nature of AI Readiness training is not the same as merely learning about AI. Rather, AI Readiness recognises the diversity of the professions, workplaces and sectors for whom AI has a potential impact. For example, AI Readiness for lawyers may be based on the same principles as AI Readiness for Educators. However, the details will be contextualised differently. AI Readiness recognises that such contextualisation is not an option: it is essential due to the multiple intricacies, sensitivities and variations between different sectors and their settings, which all impact the application of AI. To embrace such contextualisation, AI Readiness needs to be an active, participatory training process and aims to empower people to be more able to leverage AI to meet their needs. The text that follows focuses on AI Readiness within the Education and Training sector and starts with a discussion of the current state of AI within education and training, and the need for AI Readiness. We then problematize the concept of AI Readiness, why AI Readiness is needed, and what it means. We expand upon the nature of AI Readiness through a discussion of the difference between human and Artificial Intelligence, before presenting a 7-step framework for helping people to become AI Ready. Finally, we use an example of AI Readiness in action within Higher Education to exemplify AI Readiness.},
  langid = {english},
  file = {/Users/andrew/Zotero/storage/L2RNJKXY/Luckin et al. - 2022 - Empowering educators to be AI-ready.pdf}
}

@online{mahowaldDissociatingLanguageThought2023a,
  title = {Dissociating Language and Thought in Large Language Models: A Cognitive Perspective},
  shorttitle = {Dissociating Language and Thought in Large Language Models},
  author = {Mahowald, Kyle and Ivanova, Anna A. and Blank, Idan A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
  date = {2023-01-16},
  eprint = {2301.06627},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.06627},
  url = {http://arxiv.org/abs/2301.06627},
  urldate = {2023-02-12},
  abstract = {Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become -- "thinking machines", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/BGLP7GQM/Mahowald et al_2023_Dissociating language and thought in large language models.pdf;/Users/andrew/Zotero/storage/Q5CBAEVF/2301.html}
}

@article{markauskaiteRethinkingEntwinementArtificial2022,
  title = {Rethinking the Entwinement between Artificial Intelligence and Human Learning: {{What}} Capabilities Do Learners Need for a World with {{AI}}?},
  shorttitle = {Rethinking the Entwinement between Artificial Intelligence and Human Learning},
  author = {Markauskaite, Lina and Marrone, Rebecca and Poquet, Oleksandra and Knight, Simon and Martinez-Maldonado, Roberto and Howard, Sarah and Tondeur, Jo and De Laat, Maarten and Buckingham Shum, Simon and Gašević, Dragan and Siemens, George},
  date = {2022},
  journaltitle = {Computers and Education: Artificial Intelligence},
  shortjournal = {Computers and Education: Artificial Intelligence},
  volume = {3},
  pages = {100056},
  issn = {2666920X},
  doi = {10.1016/j.caeai.2022.100056},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2666920X2200011X},
  urldate = {2023-04-13},
  abstract = {The proliferation of AI in many aspects of human life—from personal leisure, to collaborative professional work, to global policy decisions—poses a sharp question about how to prepare people for an interconnected, fastchanging world which is increasingly becoming saturated with technological devices and agentic machines. What kinds of capabilities do people need in a world infused with AI? How can we conceptualise these capa­ bilities? How can we help learners develop them? How can we empirically study and assess their development? With this paper, we open the discussion by adopting a dialogical knowledge-making approach. Our team of 11 co-authors participated in an orchestrated written discussion. Engaging in a semi-independent and semi-joint written polylogue, we assembled a pool of ideas of what these capabilities are and how learners could be hel­ ped to develop them. Simultaneously, we discussed conceptual and methodological ideas that would enable us to test and refine our hypothetical views. In synthesising these ideas, we propose that there is a need to move beyond AI-centred views of capabilities and consider the ecology of technology, cognition, social interaction, and values.},
  langid = {english},
  file = {/Users/andrew/Zotero/storage/HMPQNLW7/Markauskaite et al. - 2022 - Rethinking the entwinement between artificial inte.pdf}
}

@article{markauskaiteRethinkingEntwinementArtificial2022a,
  title = {Rethinking the Entwinement between Artificial Intelligence and Human Learning: {{What}} Capabilities Do Learners Need for a World with {{AI}}?},
  shorttitle = {Rethinking the Entwinement between Artificial Intelligence and Human Learning},
  author = {Markauskaite, Lina and Marrone, Rebecca and Poquet, Oleksandra and Knight, Simon and Martinez-Maldonado, Roberto and Howard, Sarah and Tondeur, Jo and De Laat, Maarten and Buckingham Shum, Simon and Gašević, Dragan and Siemens, George},
  date = {2022-01-01},
  journaltitle = {Computers and Education: Artificial Intelligence},
  shortjournal = {Computers and Education: Artificial Intelligence},
  volume = {3},
  pages = {100056},
  issn = {2666-920X},
  doi = {10.1016/j.caeai.2022.100056},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X2200011X},
  urldate = {2023-04-26},
  abstract = {The proliferation of AI in many aspects of human life—from personal leisure, to collaborative professional work, to global policy decisions—poses a sharp question about how to prepare people for an interconnected, fast-changing world which is increasingly becoming saturated with technological devices and agentic machines. What kinds of capabilities do people need in a world infused with AI? How can we conceptualise these capabilities? How can we help learners develop them? How can we empirically study and assess their development? With this paper, we open the discussion by adopting a dialogical knowledge-making approach. Our team of 11 co-authors participated in an orchestrated written discussion. Engaging in a semi-independent and semi-joint written polylogue, we assembled a pool of ideas of what these capabilities are and how learners could be helped to develop them. Simultaneously, we discussed conceptual and methodological ideas that would enable us to test and refine our hypothetical views. In synthesising these ideas, we propose that there is a need to move beyond AI-centred views of capabilities and consider the ecology of technology, cognition, social interaction, and values.},
  langid = {english},
  keywords = {AI in education,Capabilities for AI,Ecological approach,Postdigital dialogue},
  file = {/Users/andrew/Zotero/storage/MJH9X3KD/Markauskaite et al_2022_Rethinking the entwinement between artificial intelligence and human learning.pdf;/Users/andrew/Zotero/storage/EX4MSQMH/S2666920X2200011X.html}
}

@online{mitchellDebateUnderstandingAI2023,
  title = {The {{Debate Over Understanding}} in {{AI}}'s {{Large Language Models}}},
  author = {Mitchell, Melanie and Krakauer, David C.},
  date = {2023-02-10},
  eprint = {2210.13966},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.13966},
  url = {http://arxiv.org/abs/2210.13966},
  urldate = {2023-03-29},
  abstract = {We survey a current, heated debate in the AI research community on whether large pre-trained language models can be said to "understand" language -- and the physical and social situations language encodes -- in any important sense. We describe arguments that have been made for and against such understanding, and key questions for the broader sciences of intelligence that have arisen in light of these arguments. We contend that a new science of intelligence can be developed that will provide insight into distinct modes of understanding, their strengths and limitations, and the challenge of integrating diverse forms of cognition.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/WAVN8MYY/Mitchell_Krakauer_2023_The Debate Over Understanding in AI's Large Language Models.pdf;/Users/andrew/Zotero/storage/XAYUWIJI/2210.html}
}

@online{mitchellDetectGPTZeroShotMachineGenerated2023,
  title = {{{DetectGPT}}: {{Zero-Shot Machine-Generated Text Detection}} Using {{Probability Curvature}}},
  shorttitle = {{{DetectGPT}}},
  author = {Mitchell, Eric and Lee, Yoonho and Khazatsky, Alexander and Manning, Christopher D. and Finn, Chelsea},
  date = {2023-01-26},
  eprint = {2301.11305},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2301.11305},
  urldate = {2023-02-15},
  abstract = {The fluency and factual knowledge of large language models (LLMs) heightens the need for corresponding systems to detect whether a piece of text is machine-written. For example, students may use LLMs to complete written assignments, leaving instructors unable to accurately assess student learning. In this paper, we first demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g, T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/VFZ4ZU2A/Mitchell et al_2023_DetectGPT.pdf;/Users/andrew/Zotero/storage/HJEJ9TSC/2301.html}
}

@online{mollickNewModesLearning2022a,
  type = {SSRN Scholarly Paper},
  title = {New {{Modes}} of {{Learning Enabled}} by {{AI Chatbots}}: {{Three Methods}} and {{Assignments}}},
  shorttitle = {New {{Modes}} of {{Learning Enabled}} by {{AI Chatbots}}},
  author = {Mollick, Ethan R. and Mollick, Lilach},
  date = {2022-12-13},
  number = {4300783},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.4300783},
  url = {https://papers.ssrn.com/abstract=4300783},
  urldate = {2023-02-16},
  abstract = {Chatbots are able to produce high-quality, sophisticated text in natural language. The authors of this paper believe that AI can be used to overcome three barriers to learning in the classroom: improving transfer, breaking the illusion of explanatory depth, and training students to critically evaluate explanations. The paper provides background information and techniques on how AI can be used to overcome these barriers and includes prompts and assignments that teachers can incorporate into their teaching. The goal is to help teachers use the capabilities and drawbacks of AI to improve learning},
  langid = {english},
  pubstate = {preprint},
  keywords = {/unread,AI,chatbot,education,learning,transfer},
  file = {/Users/andrew/Zotero/storage/TI6A8P95/Mollick_Mollick_2022_New Modes of Learning Enabled by AI Chatbots.pdf}
}

@article{mollickUsingAIImplement2023,
  title = {Using {{AI}} to {{Implement Effective Teaching Strategies}} in {{Classrooms}}: {{Five Strategies}}, {{Including Prompts}}},
  shorttitle = {Using {{AI}} to {{Implement Effective Teaching Strategies}} in {{Classrooms}}},
  author = {Mollick, Ethan R. and Mollick, Lilach},
  date = {2023},
  journaltitle = {SSRN Electronic Journal},
  shortjournal = {SSRN Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4391243},
  url = {https://www.ssrn.com/abstract=4391243},
  urldate = {2023-05-03},
  abstract = {This paper provides guidance for using AI to quickly and easily implement evidencebased teaching strategies that instructors can integrate into their teaching. We discuss five teaching strategies that have proven value but are hard to implement in practice due to time and effort constraints. We show how AI can help instructors create material that supports these strategies and improve student learning. The strategies include providing multiple examples and explanations; uncovering and addressing student misconceptions; frequent low-stakes testing; assessing student learning; and distributed practice. The paper provides guidelines for how AI can support each strategy, and discusses both the promises and perils of this approach, arguing that AI may act as a “force multiplier” for instructors if implemented cautiously and thoughtfully in service of evidence-based teaching practices.},
  langid = {english},
  file = {/Users/andrew/Zotero/storage/ZWU886LV/Mollick and Mollick - 2023 - Using AI to Implement Effective Teaching Strategie.pdf}
}

@online{ouyangTrainingLanguageModels2022b,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  date = {2022-03-04},
  eprint = {2203.02155},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.02155},
  urldate = {2023-02-13},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/FAADUUEG/Ouyang et al_2022_Training language models to follow instructions with human feedback.pdf;/Users/andrew/Zotero/storage/WMCGN5Y8/2203.html}
}

@inproceedings{pardosOATutorOpensourceAdaptive2023,
  title = {{{OATutor}}: {{An Open-source Adaptive Tutoring System}} and {{Curated Content Library}} for {{Learning Sciences Research}}},
  shorttitle = {{{OATutor}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Pardos, Zachary A. and Tang, Matthew and Anastasopoulos, Ioannis and Sheel, Shreya K. and Zhang, Ethan},
  date = {2023-04-19},
  series = {{{CHI}} '23},
  pages = {1--17},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3544548.3581574},
  url = {https://dl.acm.org/doi/10.1145/3544548.3581574},
  urldate = {2023-04-27},
  abstract = {Despite decades long establishment of effective tutoring principles, no adaptive tutoring system has been developed and open-sourced to the research community. The absence of such a system inhibits researchers from replicating adaptive learning studies and extending and experimenting with various tutoring system design directions. For this reason, adaptive learning research is primarily conducted on a small number of proprietary platforms. In this work, we aim to democratize adaptive learning research with the introduction of the first open-source adaptive tutoring system based on Intelligent Tutoring System principles. The system, we call Open Adaptive Tutor (OATutor), has been iteratively developed over three years with field trials in classrooms drawing feedback from students, teachers, and researchers. The MIT-licensed source code includes three creative commons (CC BY) textbooks worth of algebra problems, with tutoring supports authored by the OATutor project. Knowledge Tracing, an A/B testing framework, and LTI support are included.},
  isbn = {978-1-4503-9421-5},
  keywords = {Adaptive learning,content authoring,intelligent tutoring systems,OER,open source,replicable research,research through design},
  file = {/Users/andrew/Zotero/storage/CXRWYD97/Pardos et al_2023_OATutor.pdf}
}

@article{peachUnderstandingLearnerBehaviour2021,
  title = {Understanding Learner Behaviour in Online Courses with {{Bayesian}} Modelling and Time Series Characterisation},
  author = {Peach, Robert L. and Greenbury, Sam F. and Johnston, Iain G. and Yaliraki, Sophia N. and Lefevre, David J. and Barahona, Mauricio},
  date = {2021-02-02},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {11},
  number = {1},
  pages = {2823},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-81709-3},
  url = {https://www.nature.com/articles/s41598-021-81709-3},
  urldate = {2023-05-24},
  abstract = {The intrinsic temporality of learning demands the adoption of methodologies capable of exploiting time-series information. In this study we leverage the sequence data framework and show how data-driven analysis of temporal sequences of task completion in online courses can be used to characterise personal and group learners’ behaviors, and to identify critical tasks and course sessions in a given course design. We also introduce a recently developed probabilistic Bayesian model to learn sequential behaviours of students and predict student performance. The application of our data-driven sequence-based analyses to data from learners undertaking an on-line Business Management course reveals distinct behaviors within the cohort of learners, identifying learners or groups of learners that deviate from the nominal order expected in the course. Using course grades a posteriori, we explore differences in behavior between high and low performing learners. We find that high performing learners follow the progression between weekly sessions more regularly than low performing learners, yet within each weekly session high performing learners are less tied to the nominal task order. We then model the sequences of high and low performance students using the probablistic Bayesian model and show that we can learn engagement behaviors associated with performance. We also show that the data sequence framework can be used for task-centric analysis; we identify critical junctures and differences among types of tasks within the course design. We find that non-rote learning tasks, such as interactive tasks or discussion posts, are correlated with higher performance. We discuss the application of such analytical techniques as an aid to course design, intervention, and student supervision.},
  issue = {1},
  langid = {english},
  keywords = {Human behaviour,Information technology},
  file = {/Users/andrew/Zotero/storage/T3ECNUX2/Peach et al_2021_Understanding learner behaviour in online courses with Bayesian modelling and.pdf}
}

@article{piantadosiModernLanguageModels,
  title = {Modern Language Models Refute {{Chomsky}}’s Approach to Language},
  author = {Piantadosi, Steven T},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/ER5CQ64P/Piantadosi - Modern language models refute Chomsky’s approach t.pdf}
}

@online{piantasodiMeaningReferenceLarge2022,
  title = {Meaning without Reference in Large Language Models},
  author = {Piantasodi, Steven T. and Hill, Felix},
  date = {2022-08-04},
  eprint = {2208.02957},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2208.02957},
  url = {http://arxiv.org/abs/2208.02957},
  urldate = {2022-08-09},
  abstract = {The widespread success of large language models (LLMs) has been met with skepticism that they possess anything like human concepts or meanings. Contrary to claims that LLMs possess no meaning whatsoever, we argue that they likely capture important aspects of meaning, and moreover work in a way that approximates a compelling account of human cognition in which meaning arises from conceptual role. Because conceptual role is defined by the relationships between internal representational states, meaning cannot be determined from a model's architecture, training data, or objective function, but only by examination of how its internal states relate to each other. This approach may clarify why and how LLMs are so successful and suggest how they can be made more human-like.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/ZWYCJ33Q/Piantasodi and Hill - 2022 - Meaning without reference in large language models.pdf;/Users/andrew/Zotero/storage/F9JAXQ23/2208.html}
}

@online{piechDeepKnowledgeTracing2015b,
  title = {Deep {{Knowledge Tracing}}},
  author = {Piech, Chris and Spencer, Jonathan and Huang, Jonathan and Ganguli, Surya and Sahami, Mehran and Guibas, Leonidas and Sohl-Dickstein, Jascha},
  date = {2015-06-19},
  eprint = {1506.05908},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1506.05908},
  url = {http://arxiv.org/abs/1506.05908},
  urldate = {2023-02-16},
  abstract = {Knowledge tracing---where a machine models the knowledge of a student as they interact with coursework---is a well established problem in computer supported education. Though effectively modeling student knowledge would have high educational impact, the task has many inherent challenges. In this paper we explore the utility of using Recurrent Neural Networks (RNNs) to model student learning. The RNN family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge, and can capture more complex representations of student knowledge. Using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets. Moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks. These results suggest a promising new line of research for knowledge tracing and an exemplary application task for RNNs.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning,K.3.1},
  file = {/Users/andrew/Zotero/storage/76MLAPTN/Piech et al_2015_Deep Knowledge Tracing.pdf;/Users/andrew/Zotero/storage/8LGWIZUL/1506.html}
}

@article{pintrichRoleMetacognitiveKnowledge2002,
  title = {The {{Role}} of {{Metacognitive Knowledge}} in {{Learning}}, {{Teaching}}, and {{Assessing}}},
  author = {Pintrich, Paul R.},
  date = {2002-11-01},
  journaltitle = {Theory Into Practice},
  shortjournal = {Theory Into Practice},
  volume = {41},
  number = {4},
  pages = {219--225},
  issn = {0040-5841, 1543-0421},
  doi = {10.1207/s15430421tip4104_3},
  url = {https://www.tandfonline.com/doi/full/10.1207/s15430421tip4104_3},
  urldate = {2023-05-05},
  langid = {english},
  file = {/Users/andrew/Zotero/storage/LJXIFAG5/Pintrich - 2002 - The Role of Metacognitive Knowledge in Learning, T.pdf;/Users/andrew/Zotero/storage/PQIEADTF/Pintrich_2002_The Role of Metacognitive Knowledge in Learning, Teaching, and Assessing.pdf}
}

@online{qinChatGPTGeneralPurposeNatural2023,
  title = {Is {{ChatGPT}} a {{General-Purpose Natural Language Processing Task Solver}}?},
  author = {Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
  date = {2023-02-15},
  eprint = {2302.06476},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.06476},
  urldate = {2023-02-22},
  abstract = {Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/QJE4EGUH/Qin et al_2023_Is ChatGPT a General-Purpose Natural Language Processing Task Solver.pdf;/Users/andrew/Zotero/storage/BQF7N8SL/2302.html}
}

@online{razeghiImpactPretrainingTerm2022,
  title = {Impact of {{Pretraining Term Frequencies}} on {{Few-Shot Reasoning}}},
  author = {Razeghi, Yasaman and Logan IV, Robert L. and Gardner, Matt and Singh, Sameer},
  date = {2022-05-23},
  eprint = {2202.07206},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2202.07206},
  urldate = {2023-02-11},
  abstract = {Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more prevalent, in some cases above \$70\textbackslash\%\$ (absolute) more accurate on the top 10\textbackslash\% frequent terms in comparison to the bottom 10\textbackslash\%. Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/HH5T9WLG/Razeghi et al_2022_Impact of Pretraining Term Frequencies on Few-Shot Reasoning.pdf;/Users/andrew/Zotero/storage/UQQ78JRW/2202.html}
}

@article{rogersPrimerBERTologyWhat2020,
  title = {A {{Primer}} in {{BERTology}}: {{What We Know About How BERT Works}}},
  shorttitle = {A {{Primer}} in {{BERTology}}},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  date = {2020},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {842--866},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA}},
  doi = {10.1162/tacl_a_00349},
  url = {https://aclanthology.org/2020.tacl-1.54},
  urldate = {2023-03-15},
  abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/2IFRFAD2/Rogers et al_2020_A Primer in BERTology.pdf}
}

@article{rohrerThatLotProcess2022,
  title = {That’s a {{Lot}} to {{Process}}! {{Pitfalls}} of {{Popular Path Models}}},
  author = {Rohrer, Julia M. and Hünermund, Paul and Arslan, Ruben C. and Elson, Malte},
  date = {2022-04-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {5},
  number = {2},
  pages = {25152459221095827},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/25152459221095827},
  url = {https://doi.org/10.1177/25152459221095827},
  urldate = {2023-05-29},
  abstract = {Path models to test claims about mediation and moderation are a staple of psychology. But applied researchers may sometimes not understand the underlying causal inference problems and thus endorse conclusions that rest on unrealistic assumptions. In this article, we aim to provide a clear explanation for the limited conditions under which standard procedures for mediation and moderation analysis can succeed. We discuss why reversing arrows or comparing model fit indices cannot tell us which model is the right one and how tests of conditional independence can at least tell us where our model goes wrong. Causal modeling practices in psychology are far from optimal but may be kept alive by domain norms that demand every article makes some novel claim about processes and boundary conditions. We end with a vision for a different research culture in which causal inference is pursued in a much slower, more deliberate, and collaborative manner.},
  langid = {english},
  file = {/Users/andrew/Zotero/storage/XPHMPG4C/Rohrer et al_2022_That’s a Lot to Process.pdf}
}

@online{schaefferAreEmergentAbilities2023,
  title = {Are {{Emergent Abilities}} of {{Large Language Models}} a {{Mirage}}?},
  author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  date = {2023-04-28},
  eprint = {2304.15004},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.15004},
  url = {http://arxiv.org/abs/2304.15004},
  urldate = {2023-05-01},
  abstract = {Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, one can choose a metric which leads to the inference of an emergent ability or another metric which does not. Thus, our alternative suggests that existing claims of emergent abilities are creations of the researcher's analyses, not fundamental changes in model behavior on specific tasks with scale. We present our explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show how similar metric decisions suggest apparent emergent abilities on vision tasks in diverse deep network architectures (convolutional, autoencoder, transformers). In all three analyses, we find strong supporting evidence that emergent abilities may not be a fundamental property of scaling AI models.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/andrew/Zotero/storage/YI3D3JAV/Schaeffer et al_2023_Are Emergent Abilities of Large Language Models a Mirage.pdf;/Users/andrew/Zotero/storage/C9ACQRBT/2304.html}
}

@article{shahExplainableKnowledgeTracing,
  title = {Explainable {{Knowledge Tracing Models}} for {{Big Data}}: {{Is Ensembling}} an {{Answer}}?},
  author = {Shah, Tirth and Sharma, Aditya and Olson, Lukas and Patel, Nirmal},
  abstract = {In this paper, we describe our Knowledge Tracing model for the 2020 NeurIPS Education Challenge. We used a combination of 22 models to predict whether the students will answer a given question correctly or not. Our combination of different approaches allowed us to get an accuracy higher than any of the individual models, and the variation of our model types gave our solution better explainability, more alignment with learning science theories, and high predictive power.},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/MZ3WIFI2/Shah et al. - Explainable Knowledge Tracing Models for Big Data.pdf}
}

@online{shanahanRolePlayLargeLanguage2023,
  title = {Role-{{Play}} with {{Large Language Models}}},
  author = {Shanahan, Murray and McDonell, Kyle and Reynolds, Laria},
  date = {2023-05-25},
  eprint = {2305.16367},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.16367},
  url = {http://arxiv.org/abs/2305.16367},
  urldate = {2023-05-29},
  abstract = {As dialogue agents become increasingly human-like in their performance, it is imperative that we develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. In this paper, we foreground the concept of role-play. Casting dialogue agent behaviour in terms of role-play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models they in fact lack. Two important cases of dialogue agent behaviour are addressed this way, namely (apparent) deception and (apparent) self-awareness.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/PN6NU6L7/Shanahan et al_2023_Role-Play with Large Language Models.pdf;/Users/andrew/Zotero/storage/YTHKAW75/2305.html}
}

@online{shanahanTalkingLargeLanguage2023,
  title = {Talking {{About Large Language Models}}},
  author = {Shanahan, Murray},
  date = {2023-01-25},
  eprint = {2212.03551},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.03551},
  url = {http://arxiv.org/abs/2212.03551},
  urldate = {2023-02-11},
  abstract = {Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as "knows", "believes", and "thinks", when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/EK8WVTU2/Shanahan_2023_Talking About Large Language Models.pdf;/Users/andrew/Zotero/storage/TDJQ7QT9/2212.html}
}

@article{tschiatschekEquityFairnessBayesian2022a,
  title = {Equity and {{Fairness}} of {{Bayesian Knowledge Tracing}}},
  author = {Tschiatschek, Sebastian and Knobelsdorf, Maria and Adish Singla},
  editora = {Mitrovic, Antonija and Bosch, Nigel},
  editoratype = {collaborator},
  date = {2022-07-18},
  publisher = {{Zenodo}},
  doi = {10.5281/ZENODO.6853012},
  url = {https://zenodo.org/record/6853012},
  urldate = {2023-03-08},
  abstract = {We consider the equity and fairness of curricula derived from Knowledge Tracing models. We begin by defining a unifying notion of an equitable tutoring system as a system that achieves maximum possible knowledge in minimal time for each student interacting with it. Realizing perfect equity requires tutoring systems that can provide individualized curricula per student. In particular, we investigate the design of equitable tutoring systems that derive their curricula from Knowledge Tracing models. We first show that the classical Bayesian Knowledge Tracing (BKT) model and their derived curricula can fall short of achieving equitable tutoring. To overcome this issue, we then propose a novel model, Bayesian-Bayesian Knowledge Tracing (B2KT), that naturally allows online individualization. We demonstrate that curricula derived from our model are more effective and equitable than those derived from existing models. Furthermore, we highlight that improving models with a focus on the fairness of next-step predictions can be insufficient to develop equitable tutoring systems.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/QGCP2NZ7/Tschiatschek et al. - 2022 - Equity and Fairness of Bayesian Knowledge Tracing.pdf}
}

@article{turingCOMPUTINGMACHINERYINTELLIGENCE1950,
  title = {I.—{{COMPUTING MACHINERY AND INTELLIGENCE}}},
  author = {TURING, A. M.},
  date = {1950-10-01},
  journaltitle = {Mind},
  shortjournal = {Mind},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  issn = {0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  url = {https://doi.org/10.1093/mind/LIX.236.433},
  urldate = {2023-05-04},
  file = {/Users/andrew/Zotero/storage/ENA265IY/TURING - 1950 - I.—COMPUTING MACHINERY AND INTELLIGENCE.pdf;/Users/andrew/Zotero/storage/NS3QIJKK/TURING_1950_I.pdf}
}

@article{universityoftasmaniaaustraliaPromptingHigherEducation2023,
  title = {Prompting {{Higher Education Towards AI-Augmented Teaching}} and {{Learning Practice}}},
  author = {{University of Tasmania, Australia} and Eager, Bronwyn and Brunton, Ryan and {University of Tasmania, Australia}},
  date = {2023-05-29},
  journaltitle = {Journal of University Teaching and Learning Practice},
  shortjournal = {JUTLP},
  volume = {20},
  number = {5},
  issn = {14499789, 14499789},
  doi = {10.53761/1.20.5.02},
  url = {https://ro.uow.edu.au/jutlp/vol20/iss5/02/},
  urldate = {2023-05-30},
  abstract = {Large Language Models (LLMs) and conversational-style generative artificial intelligence (AI) are causing major disruption to higher education pedagogy. The emergence of tools like ChatGPT has raised concerns about plagiarism detection but also presents opportunities for educators to leverage AI to build supportive learning environments. In this commentary, we explore the potential of AI-augmented teaching and learning practice in higher education, discussing both the productive affordances and challenges associated with these technologies. We offer instructional advice for writing instructional text to guide the generation of quality outputs from AI models, as well as a case study to illustrate using AI for assessment design. Ultimately, we suggest that AI should be seen as one tool among many that can be used to enhance teaching and learning outcomes in higher education.},
  langid = {english},
  file = {/Users/andrew/Zotero/storage/99UM9AJ2/University of Tasmania, Australia et al. - 2023 - Prompting Higher Education Towards AI-Augmented Te.pdf}
}

@unpublished{vaswaniAttentionAllYou2017b,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2022-05-15},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/JKYDXK2K/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/andrew/Zotero/storage/2DZV3P2H/1706.html}
}

@article{velezTeachersRecruitMentalizing2023,
  title = {Teachers Recruit Mentalizing Regions to Represent Learners’ Beliefs},
  author = {Vélez, Natalia and Chen, Alicia M. and Burke, Taylor and Cushman, Fiery A. and Gershman, Samuel J.},
  date = {2023-05-30},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {22},
  pages = {e2215015120},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.2215015120},
  url = {https://www.pnas.org/doi/10.1073/pnas.2215015120},
  urldate = {2023-05-24},
  abstract = {Teaching enables humans to impart vast stores of culturally specific knowledge and skills. However, little is known about the neural computations that guide teachers’ decisions about what information to communicate. Participants (N = 28) played the role of teachers while being scanned using fMRI; their task was to select examples that would teach learners how to answer abstract multiple-choice questions. Participants’ examples were best described by a model that selects evidence that maximizes the learner’s belief in the correct answer. Consistent with this idea, participants’ predictions about how well learners would do closely tracked the performance of an independent sample of learners (N = 140) who were tested on the examples they had provided. In addition, regions that play specialized roles in processing social information, namely the bilateral temporoparietal junction and middle and dorsal medial prefrontal cortex, tracked learners’ posterior belief in the correct answer. Our results shed light on the computational and neural architectures that support our extraordinary abilities as teachers.}
}

@online{wangInstructionsGuideDiagnostic2021,
  title = {Instructions and {{Guide}} for {{Diagnostic Questions}}: {{The NeurIPS}} 2020 {{Education Challenge}}},
  shorttitle = {Instructions and {{Guide}} for {{Diagnostic Questions}}},
  author = {Wang, Zichao and Lamb, Angus and Saveliev, Evgeny and Cameron, Pashmina and Zaykov, Yordan and Hernández-Lobato, José Miguel and Turner, Richard E. and Baraniuk, Richard G. and Barton, Craig and Jones, Simon Peyton and Woodhead, Simon and Zhang, Cheng},
  date = {2021-04-12},
  eprint = {2007.12061},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2007.12061},
  url = {http://arxiv.org/abs/2007.12061},
  urldate = {2023-03-28},
  abstract = {Digital technologies are becoming increasingly prevalent in education, enabling personalized, high quality education resources to be accessible by students across the world. Importantly, among these resources are diagnostic questions: the answers that the students give to these questions reveal key information about the specific nature of misconceptions that the students may hold. Analyzing the massive quantities of data stemming from students' interactions with these diagnostic questions can help us more accurately understand the students' learning status and thus allow us to automate learning curriculum recommendations. In this competition, participants will focus on the students' answer records to these multiple-choice diagnostic questions, with the aim of 1) accurately predicting which answers the students provide; 2) accurately predicting which questions have high quality; and 3) determining a personalized sequence of questions for each student that best predicts the student's answers. These tasks closely mimic the goals of a real-world educational platform and are highly representative of the educational challenges faced today. We provide over 20 million examples of students' answers to mathematics questions from Eedi, a leading educational platform which thousands of students interact with daily around the globe. Participants to this competition have a chance to make a lasting, real-world impact on the quality of personalized education for millions of students across the world.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/NSU32T9U/Wang et al_2021_Instructions and Guide for Diagnostic Questions.pdf;/Users/andrew/Zotero/storage/LEB5VMBT/2007.html}
}

@online{wangSelfConsistencyImprovesChain2023,
  title = {Self-{{Consistency Improves Chain}} of {{Thought Reasoning}} in {{Language Models}}},
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  date = {2023-03-07},
  eprint = {2203.11171},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.11171},
  url = {http://arxiv.org/abs/2203.11171},
  urldate = {2023-06-09},
  abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/KHTHU6S9/Wang et al_2023_Self-Consistency Improves Chain of Thought Reasoning in Language Models.pdf;/Users/andrew/Zotero/storage/MHHCVKHB/2203.html}
}

@online{wangVarFAVariationalFactor2020,
  title = {{{VarFA}}: {{A Variational Factor Analysis Framework For Efficient Bayesian Learning Analytics}}},
  shorttitle = {{{VarFA}}},
  author = {Wang, Zichao and Gu, Yi and Lan, Andrew and Baraniuk, Richard},
  date = {2020-08-14},
  eprint = {2005.13107},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2005.13107},
  url = {http://arxiv.org/abs/2005.13107},
  urldate = {2023-05-24},
  abstract = {We propose VarFA, a variational inference factor analysis framework that extends existing factor analysis models for educational data mining to efficiently output uncertainty estimation in the model's estimated factors. Such uncertainty information is useful, for example, for an adaptive testing scenario, where additional tests can be administered if the model is not quite certain about a students' skill level estimation. Traditional Bayesian inference methods that produce such uncertainty information are computationally expensive and do not scale to large data sets. VarFA utilizes variational inference which makes it possible to efficiently perform Bayesian inference even on very large data sets. We use the sparse factor analysis model as a case study and demonstrate the efficacy of VarFA on both synthetic and real data sets. VarFA is also very general and can be applied to a wide array of factor analysis models.},
  pubstate = {preprint},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning},
  file = {/Users/andrew/Zotero/storage/7JNRD8KS/Wang et al_2020_VarFA.pdf;/Users/andrew/Zotero/storage/F6N6SDAL/2005.html}
}

@online{weiChainofThoughtPromptingElicits2023,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  date = {2023-01-10},
  eprint = {2201.11903},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.11903},
  url = {http://arxiv.org/abs/2201.11903},
  urldate = {2023-03-02},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/GJBNDTL9/Wei et al_2023_Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf;/Users/andrew/Zotero/storage/TJEZ8D29/2201.html}
}

@online{weiEmergentAbilitiesLarge2022,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  date = {2022-10-26},
  eprint = {2206.07682},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.07682},
  url = {http://arxiv.org/abs/2206.07682},
  urldate = {2023-04-20},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/EHJETBL3/Wei et al_2022_Emergent Abilities of Large Language Models.pdf;/Users/andrew/Zotero/storage/BMBRKP3L/2206.html}
}

@online{weiEmergentAbilitiesLarge2022a,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  date = {2022-10-26},
  eprint = {2206.07682},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2206.07682},
  urldate = {2023-04-25},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence raises the question of whether additional scaling could potentially further expand the range of capabilities of language models.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/NN37SV4E/Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf}
}

@inproceedings{weitekampInteractionDesignMachine2020,
  title = {An {{Interaction Design}} for {{Machine Teaching}} to {{Develop AI Tutors}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Weitekamp, Daniel and Harpstead, Erik and Koedinger, Ken R.},
  date = {2020-04-23},
  series = {{{CHI}} '20},
  pages = {1--11},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3313831.3376226},
  url = {https://doi.org/10.1145/3313831.3376226},
  urldate = {2023-03-17},
  abstract = {Intelligent tutoring systems (ITSs) have consistently been shown to improve the educational outcomes of students when used alone or combined with traditional instruction. However, building an ITS is a time-consuming process which requires specialized knowledge of existing tools. Extant authoring methods, including the Cognitive Tutor Authoring Tools' (CTAT) example-tracing method and SimStudent's Authoring by Tutoring, use programming-by-demonstration to allow authors to build ITSs more quickly than they could by hand programming with model-tracing. Yet these methods still suffer from long authoring times or difficulty creating complete models. In this study, we demonstrate that Simulated Learners built with the Apprentice Learner (AL) Framework can be combined with a novel interaction design that emphasizes model transparency, input flexibility, and problem solving control to enable authors to achieve greater model completeness in less time than existing authoring methods.},
  isbn = {978-1-4503-6708-0},
  keywords = {\{simulated learners\vphantom\},/unread,intelligent tutoring systems,interaction design,machine teaching,programming-by-demonstration},
  file = {/Users/andrew/Zotero/storage/SSJXVDJL/Weitekamp et al_2020_An Interaction Design for Machine Teaching to Develop AI Tutors.pdf}
}

@online{WhatChatGPTDoing2023,
  title = {What {{Is ChatGPT Doing}} … and {{Why Does It Work}}?},
  date = {2023-02-14},
  url = {https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/},
  urldate = {2023-04-20},
  abstract = {Stephen Wolfram explores the broader picture of what's going on inside ChatGPT and why it produces meaningful text. Discusses models, training neural nets, embeddings, tokens, transformers, language syntax.},
  langid = {english},
  file = {/Users/andrew/Zotero/storage/3G5VCMNQ/what-is-chatgpt-doing-and-why-does-it-work.html}
}

@online{wolframWhatChatGPTDoing2023,
  title = {What {{Is ChatGPT Doing}} … and {{Why Does It Work}}?},
  author = {Wolfram, Stephen},
  date = {2023-02-14},
  url = {https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/},
  urldate = {2023-03-01},
  abstract = {Stephen Wolfram explores the broader picture of what's going on inside ChatGPT and why it produces meaningful text. Discusses models, training neural nets, embeddings, tokens, transformers, language syntax.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/SYQP8GVU/what-is-chatgpt-doing-and-why-does-it-work.html}
}

@article{xuePhyQMeasurePhysical2023,
  title = {Phy-{{Q}} as a Measure for Physical Reasoning Intelligence},
  author = {Xue, Cheng and Pinto, Vimukthini and Gamage, Chathura and Nikonova, Ekaterina and Zhang, Peng and Renz, Jochen},
  date = {2023-01},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {5},
  number = {1},
  pages = {83--93},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00583-4},
  url = {https://www.nature.com/articles/s42256-022-00583-4},
  urldate = {2023-02-11},
  abstract = {Humans are well versed in reasoning about the behaviours of physical objects and choosing actions accordingly to accomplish tasks, while this remains a major challenge for artificial intelligence. To facilitate research addressing this problem, we propose a new testbed that requires an agent to reason about physical scenarios and take an action appropriately. Inspired by the physical knowledge acquired in infancy and the capabilities required for robots to operate in real-world environments, we identify 15 essential physical scenarios. We create a wide variety of distinct task templates, and we ensure that all the task templates within the same scenario can be solved by using one specific strategic physical rule. By having such a design, we evaluate two distinct levels of generalization, namely local generalization and broad generalization. We conduct an extensive evaluation with human players, learning agents with various input types and architectures, and heuristic agents with different strategies. Inspired by how the human intelligence quotient is calculated, we define the physical reasoning quotient (Phy-Q score) that reflects the physical reasoning intelligence of an agent using the physical scenarios we considered. Our evaluation shows that (1) all the agents are far below human performance, and (2) learning agents, even with good local generalization ability, struggle to learn the underlying physical reasoning rules and fail to generalize broadly. We encourage the development of intelligent agents that can reach the human-level Phy-Q score.},
  issue = {1},
  langid = {english},
  keywords = {/unread,Computer science,Human behaviour,Scientific data,Software},
  file = {/Users/andrew/Zotero/storage/T8SET792/Xue et al_2023_Phy-Q as a measure for physical reasoning intelligence.pdf}
}

@article{yangOneModelLearning2022,
  title = {One Model for the Learning of Language},
  author = {Yang, Yuan and Piantadosi, Steven T.},
  date = {2022-02},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {119},
  number = {5},
  pages = {e2021865119},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2021865119},
  url = {https://pnas.org/doi/full/10.1073/pnas.2021865119},
  urldate = {2023-03-14},
  abstract = {Significance             It has long been hypothesized that language acquisition may be impossible without innate knowledge of the structures that occur in natural language. Here, we show that a domain general learning setup, originally developed in cognitive psychology to model rule learning, is able to acquire key pieces of natural language from relatively few examples of sentences. This develops a new approach to formalizing linguistic learning and highlights some features of language and language acquisition that may arise from general cognitive processes.           ,                             A major goal of linguistics and cognitive science is to understand what class of learning systems can acquire natural language. Until recently, the computational requirements of language have been used to argue that learning is impossible without a highly constrained hypothesis space. Here, we describe a learning system that is maximally unconstrained, operating over the space of all computations, and is able to acquire many of the key structures present in natural language from positive evidence alone. We demonstrate this by providing the same learning model with data from 74 distinct formal languages which have been argued to capture key features of language, have been studied in experimental work, or come from an interesting complexity class. The model is able to successfully induce the latent system generating the observed strings from small amounts of evidence in almost all cases, including for regular (e.g.,                                a                 n                              ,                                                                                                                        (                         a                         b                         )                                              n                                                                                       , and                                                                                                                        \{                         a                         ,                         b                         \}                                              +                                                                                       ), context-free (e.g.,                                                                                               a                       n                                                                 b                       n                                          ,                     \,                                            a                       n                                                                 b                                                n                         +                         m                                                                                                              , and                                                                        x                                            x                       R                                                                                       ), and context-sensitive (e.g.,                                                                                               a                       n                                                                 b                       n                                                                 c                       n                                          ,                     \,                                            a                       n                                                                 b                       m                                                                 c                       n                                                                 d                       m                                                                                       , and               xx               ) languages, as well as for many languages studied in learning experiments. These results show that relatively small amounts of positive evidence can support learning of rich classes of generative computations over structures. The model provides an idealized learning setup upon which additional cognitive constraints and biases can be formalized.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/VWA6RP6X/Yang_Piantadosi_2022_One model for the learning of language.pdf}
}

@online{yaoTreeThoughtsDeliberate2023,
  title = {Tree of {{Thoughts}}: {{Deliberate Problem Solving}} with {{Large Language Models}}},
  shorttitle = {Tree of {{Thoughts}}},
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
  date = {2023-05-17},
  eprint = {2305.10601},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.10601},
  url = {http://arxiv.org/abs/2305.10601},
  urldate = {2023-05-23},
  abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/ysymyth/tree-of-thought-llm.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/QVACSS9L/Yao et al. - 2023 - Tree of Thoughts Deliberate Problem Solving with .pdf;/Users/andrew/Zotero/storage/Q545MW3W/2305.html}
}

@incollection{yudelsonIndividualizedBayesianKnowledge2013a,
  title = {Individualized {{Bayesian Knowledge Tracing Models}}},
  booktitle = {Artificial {{Intelligence}} in {{Education}}},
  author = {Yudelson, Michael V. and Koedinger, Kenneth R. and Gordon, Geoffrey J.},
  editor = {Lane, H. Chad and Yacef, Kalina and Mostow, Jack and Pavlik, Philip},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  editorbtype = {redactor},
  date = {2013},
  volume = {7926},
  pages = {171--180},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-39112-5_18},
  url = {http://link.springer.com/10.1007/978-3-642-39112-5_18},
  urldate = {2023-03-08},
  abstract = {Bayesian Knowledge Tracing (BKT)[1] is a user modeling method extensively used in the area of Intelligent Tutoring Systems. In the standard BKT implementation, there are only skill-specific parameters. However, a large body of research strongly suggests that studentspecific variability in the data, when accounted for, could enhance model accuracy [5, 6, 8]. In this work, we revisit the problem of introducing student-specific parameters into BKT on a larger scale. We show that student-specific parameters lead to a tangible improvement when predicting the data of unseen students, and that parameterizing students’ speed of learning is more beneficial than parameterizing a priori knowledge.},
  isbn = {978-3-642-39111-8 978-3-642-39112-5},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/CI8MTT3S/Yudelson et al. - 2013 - Individualized Bayesian Knowledge Tracing Models.pdf}
}

@incollection{yudelsonIndividualizedBayesianKnowledge2013b,
  title = {Individualized {{Bayesian Knowledge Tracing Models}}},
  booktitle = {Artificial {{Intelligence}} in {{Education}}},
  author = {Yudelson, Michael V. and Koedinger, Kenneth R. and Gordon, Geoffrey J.},
  editor = {Lane, H. Chad and Yacef, Kalina and Mostow, Jack and Pavlik, Philip},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  editorbtype = {redactor},
  date = {2013},
  volume = {7926},
  pages = {171--180},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-39112-5_18},
  url = {http://link.springer.com/10.1007/978-3-642-39112-5_18},
  urldate = {2023-03-24},
  abstract = {Bayesian Knowledge Tracing (BKT)[1] is a user modeling method extensively used in the area of Intelligent Tutoring Systems. In the standard BKT implementation, there are only skill-specific parameters. However, a large body of research strongly suggests that studentspecific variability in the data, when accounted for, could enhance model accuracy [5, 6, 8]. In this work, we revisit the problem of introducing student-specific parameters into BKT on a larger scale. We show that student-specific parameters lead to a tangible improvement when predicting the data of unseen students, and that parameterizing students’ speed of learning is more beneficial than parameterizing a priori knowledge.},
  isbn = {978-3-642-39111-8 978-3-642-39112-5},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/I7K7Z9MI/Yudelson et al. - 2013 - Individualized Bayesian Knowledge Tracing Models.pdf}
}

@inproceedings{yudelsonIndividualizedBayesianKnowledge2013c,
  title = {Individualized {{Bayesian Knowledge Tracing Models}}},
  booktitle = {Artificial {{Intelligence}} in {{Education}}},
  author = {Yudelson, Michael V. and Koedinger, Kenneth R. and Gordon, Geoffrey J.},
  editor = {Lane, H. Chad and Yacef, Kalina and Mostow, Jack and Pavlik, Philip},
  date = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {171--180},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-39112-5_18},
  abstract = {Bayesian Knowledge Tracing (BKT)[1] is a user modeling method extensively used in the area of Intelligent Tutoring Systems. In the standard BKT implementation, there are only skill-specific parameters. However, a large body of research strongly suggests that student-specific variability in the data, when accounted for, could enhance model accuracy [5,6,8]. In this work, we revisit the problem of introducing student-specific parameters into BKT on a larger scale. We show that student-specific parameters lead to a tangible improvement when predicting the data of unseen students, and that parameterizing students’ speed of learning is more beneficial than parameterizing a priori knowledge.},
  isbn = {978-3-642-39112-5},
  langid = {english},
  keywords = {/unread,Bayesian knowledge tracing,model fitting,model selection,student-specific model parameters},
  file = {/Users/andrew/Zotero/storage/EK43IS7Z/Yudelson et al_2013_Individualized Bayesian Knowledge Tracing Models.pdf}
}
